- ADMN

  Distributed optimization and statistical learning via the alternating direction method of multipliers

- ISTA

  An iterative thresholding algorithm for linear inverse problems with a sparsity constraint

- FISTA

  A fast iterative shrinkage-thresholding algorithm for linear inverse problems

- TwIST

  Two-step iterative shrinkage/thresholding algorithms for image restoration

- GPSR

  Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems

- GAP

  Generalized alternating projection for weighted-ℓ2, 1 minimization with applications to model-based compressive sensing

- TV

  （priors including sparsity) Low-cost compressive sensing for color video and depth

  Generalized alternating projection based total variation minimization for compressive sensing

- PnP

  Plug-and-play priors for model based reconstruction

  PnP (app) Rank minimization for  snapshot compressive imaging

- PnP & DL

  Plug-and-play algorithms for large-scale snapshot compressive imaging

  Deep plug-and-play priors for spectral snapshot compressive imaging

- OMP

  Orthogonal matching pursuit

- CoSaMP

  Iterative signal recovery from imcomplete and inaccurate samples

- ROMP

  Uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit

- SP

  Subspace pursuit for compressive sensing signal reconstruction

- IHT

  Iterative hard thresholding for compressed sensing

## ADMN

alternating direction method of multipliers，交替方向乘子法
$$
\mathop{\min}\limits_{x,z}f(x) + g(z)\\
s.t. Ax +Bz=c\\
x\in{\mathbb{R}^p},z\in{\mathbb{R}^q},A\in{\mathbb{R}^{m\times p},B \in \mathbb{R}^{m\times q},c\in \mathbb{R}^{k}}\\
f: \mathbb{R}^p \rightarrow \mathbb{R}, g:\mathbb{R}^q \rightarrow \mathbb{R}
$$
目标函数包含2组可分离自变量($x$和$z$)，且存在线性等式约束，先对目标函数进行**增广**，原始优化问题转化为：
$$
\mathop{\min}_{x,z}Q_\rho(x,z) = f(x) + g(z) + \frac{\rho}{2}\Vert Ax+Bz-c \Vert_2\\
$$
进一步写该问题的拉格朗日式：
$$
L_\rho (x,z,\lambda)=Q_\rho(x,z)+\lambda^T(Ax+Bz-c)\\
=f(x)+g(z) + \frac {\rho}{2}\Vert Ax+Bz -c\Vert_2 + \lambda^T(Ax+Bz-c)\\
\lambda \in \mathbb{R}^k
$$
$\lambda$为拉格朗日乘子（向量），接着开始迭代直至收敛：

1. 更新x：
   $$
   x^{(k)}=\mathop{\arg \min}_x L_\rho(x,z^{(k-1)}\lambda^{(k)})
   $$
   

![image-20240601153154692](C:\Users\z1002\AppData\Roaming\Typora\typora-user-images\image-20240601153154692.png)

![image-20240601153213685](C:\Users\z1002\AppData\Roaming\Typora\typora-user-images\image-20240601153213685.png)

## ![image-20240601153240596](C:\Users\z1002\AppData\Roaming\Typora\typora-user-images\image-20240601153240596.png)

## GPSR

梯度投影稀疏重建，gradient projection for sparse reconstruction

# 优化问题

## LASSO

Regression shrinkage and selection via the lasso（1990）

最小绝对值收敛和选择算子，Least Absolute Shrinkage and Selection Operator
$$
\hat{x} = \mathop{\arg\min}{\frac{1}{2}\Vert y-Ax \Vert_2 + \lambda \Vert \beta\Vert_1}\\
$$

## BPDN

**基追踪**，Basis Pursuit Denosing

Atomic decomposition by basis pursuit(1998)

- BP:

$$
\hat{\textbf x} = \mathop {\arg \min}_{z \in \mathbb{R}^N}{\Vert \textbf {x} \Vert_1 } \\ \text{subject to} \ Ax = y
$$

- 有噪声，二次约束的BP
  $$
  \hat{\textbf x} = \mathop {\arg \min}_{z \in \mathbb{R}^N}{\Vert \textbf {x} \Vert_1 } \\ \text{subject to} \ \Vert y- Ax \Vert_2 \leqslant \sigma
  $$

- ~~转化->基追踪降噪BPDN~~
  $$
  \hat{x} = \mathop {\arg \min}_{z\in\mathbb{R}^N}\Vert y-Ax \Vert_2 + \delta \Vert x \Vert_1
  $$

## 区别

1. 约束形式：
   - LASSO使用**L1范数**作为惩罚项，没有显式的约束
   - BPDN使用**L2范数**作为惩罚项，要求其L2范数**残差值**不超过某个给定的阈值

2. 目标函数：
   - LASSO目标函数包含**数据拟合项**和L1正则项
   - BPDN目标函数只包含L1正则项，约束条件用于控制数据拟合







# Latex

- \mathbf：加粗
- \mathop：
- \min：配合mathop使用
- \mathbb：召唤实数集R
- \in：∈
- \rightarrow：->
- \Vert {} \Vert ：范数
- &：对齐
- \textbf：加粗
- \le：小于等于
- \leqslant：小于等于II
- \ge：大于等于
- \geqslant：大于等于I
- \gt：大于
- \lt：小于





































